Quick data visualizationÂ¶
First let's see how our images look like:

# display some images for every different expression

import numpy as np
import seaborn as sns
from keras.preprocessing.image import load_img, img_to_array
import matplotlib.pyplot as plt
import os

# size of the image: 48*48 pixels
pic_size = 48

# input path for the images
base_path = "../input/images/images/"

plt.figure(0, figsize=(12,20))
cpt = 0

for expression in os.listdir(base_path + "train/"):
    for i in range(1,6):
        cpt = cpt + 1
        plt.subplot(7,5,cpt)
        img = load_img(base_path + "train/" + expression + "/" +os.listdir(base_path + "train/" + expression)[i], target_size=(pic_size, pic_size))
        plt.imshow(img, cmap="gray")

plt.tight_layout()
plt.show()
Using TensorFlow backend.

Can you guess which images are related to which expressions?

This task is quite easy for a human, but it may be a bit challenging for a predictive algorithm because:

the images have a low resolution
the faces are not in the same position
some images have text written on them
some people hide part of their faces with their hands
However all this diversity of images will contribute to make a more generalizable model.

# count number of train images for each expression

for expression in os.listdir(base_path + "train"):
    print(str(len(os.listdir(base_path + "train/" + expression))) + " " + expression + " images")
4103 fear images
436 disgust images
4982 neutral images
7164 happy images
3993 angry images
3205 surprise images
4938 sad images
The image expressions in our training dataset are pretty balanced, except for the 'disgust' category.

Setup the data generators
from keras.preprocessing.image import ImageDataGenerator

# number of images to feed into the NN for every batch
batch_size = 128

datagen_train = ImageDataGenerator()
datagen_validation = ImageDataGenerator()

train_generator = datagen_train.flow_from_directory(base_path + "train",
                                                    target_size=(pic_size,pic_size),
                                                    color_mode="grayscale",
                                                    batch_size=batch_size,
                                                    class_mode='categorical',
                                                    shuffle=True)

validation_generator = datagen_validation.flow_from_directory(base_path + "validation",
                                                    target_size=(pic_size,pic_size),
                                                    color_mode="grayscale",
                                                    batch_size=batch_size,
                                                    class_mode='categorical',
                                                    shuffle=False)
Found 28821 images belonging to 7 classes.
Found 7066 images belonging to 7 classes.
Deep learning models are trained by being fed with batches of data. Keras has a very useful class to automatically feed data from a directory: ImageDataGenerator.

It can also perform data augmentation while getting the images (randomly rotating the image, zooming, etc.). This method is often used as a way to artificially get more data when the dataset has a small size.

The function flow_from_directory() specifies how the generator should import the images (path, image size, colors, etc.).

Setup our Convolutional Neural Network (CNN)
We chose to use a Convolutional Neural Network in order to tackle this face recognition problem. Indeed this type of Neural Network (NN) is good for extracting the features of images and is widely used for image analysis subjects like image classification.

Quick reminder of what a NN is:

A Neural Network is a learning framework that consists in multiple layers of artificial neurons (nodes). Each node gets weighted input data, passes it into an activation function and outputs the result of the function:



A NN is composed of several layers of nodes:



An input layer that will get the data. The size of the input layer depends on the size of the input data.
Some hidden layers that will allow the NN to learn complex interactions within the data. A Neural Network with a lot of hidden layers is called a Deep Neural Network.
An output layer that will give the final result, for instance a class prediction. The size of this layer depends on the type of output we want to produce (e.g. how many classes do we want to predict?)
Classic NNs are usually composed of several fully connected layers. This means that every neuron of one layer is connected to every neurons of the next layer.

Convolutional Neural Networks also have Convolutional layers that apply sliding functions to group of pixels that are next to each other. Therefore those structures have a better understanding of patterns that we can observe in images. We will explain this in more details after.

Now let's define the architecture of our CNN:

from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D
from keras.models import Model, Sequential
from keras.optimizers import Adam

# number of possible label values
nb_classes = 7

# Initialising the CNN
model = Sequential()

# 1 - Convolution
model.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

# 2nd Convolution layer
model.add(Conv2D(128,(5,5), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

# 3rd Convolution layer
model.add(Conv2D(512,(3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

# 4th Convolution layer
model.add(Conv2D(512,(3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

# Flattening
model.add(Flatten())

# Fully connected layer 1st layer
model.add(Dense(256))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

# Fully connected layer 2nd layer
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

model.add(Dense(nb_classes, activation='softmax'))

opt = Adam(lr=0.0001)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
We define our CNN with the following global architecture:

4 convolutional layers
2 fully connected layers
The convolutional layers will extract relevant features from the images and the fully connected layers will focus on using these features to classify well our images. This architecture was inspired by the following work on the subject: https://github.com/jrishabh96/Facial-Expression-Recognition

Let's focus on how our convolution layers work. Each of them contain the following operations:

A convolution operator: extracts features from the input image using sliding matrices to preserve the spatial relations between the pixels. The following image summarizes how it works:
The green matrix corresponds to the raw image values. The orange sliding matrix is called a 'filter' or 'kernel'. This filter slides over the image by one pixel at each step (stride). During each step, we multiply the filter with the corresponding elements of the base matrix. There are different types of filters and each one will be able to retrieve different image features:

We apply the ReLU function to introduce non linearity in our CNN. Other functions like tanh or sigmoid could also be used, but ReLU has been found to perform better in most situations.
Pooling is used to reduce the dimensionality of each features while retaining the most important information. Like for the convolutional step, we apply a sliding function on our data. Different functions can be applied: max, sum, mean... The max function usually performs better.
We also use some common techniques for each layer:

Batch normalization: improves the performance and stability of NNs by providing inputs with zero mean and unit variance.
Dropout: reduces overfitting by randomly not updating the weights of some nodes. This helps prevent the NN from relying on one node in the layer too much.
We chose softmax as our last activation function as it is commonly used for multi-label classification.

Now that our CNN is defined, we can compile it with a few more parameters. We chose the Adam optimizer as it is one of the most computationally effective. We chose the categorical cross-entropy as our loss function as it is quite relevant for classification tasks. Our metric will be the accuracy, which is also quite informative for classification tasks on balanced datasets.

Train the model
Everything is set up, let's train our model now!

%%time

# number of epochs to train the NN
epochs = 50

from keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint("model_weights.h5", monitor='val_acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

history = model.fit_generator(generator=train_generator,
                                steps_per_epoch=train_generator.n//train_generator.batch_size,
                                epochs=epochs,
                                validation_data = validation_generator,
                                validation_steps = validation_generator.n//validation_generator.batch_size,
                                callbacks=callbacks_list
                                )
Epoch 1/50
225/225 [==============================] - 76s 336ms/step - loss: 2.0135 - acc: 0.2345 - val_loss: 1.7154 - val_acc: 0.3230

Epoch 00001: val_acc improved from -inf to 0.32301, saving model to model_weights.h5
Epoch 2/50
225/225 [==============================] - 31s 138ms/step - loss: 1.8255 - acc: 0.2952 - val_loss: 1.6733 - val_acc: 0.3341

Epoch 00002: val_acc improved from 0.32301 to 0.33410, saving model to model_weights.h5
Epoch 3/50
225/225 [==============================] - 31s 137ms/step - loss: 1.7187 - acc: 0.3375 - val_loss: 1.6328 - val_acc: 0.3693

Epoch 00003: val_acc improved from 0.33410 to 0.36927, saving model to model_weights.h5
Epoch 4/50
225/225 [==============================] - 32s 141ms/step - loss: 1.6341 - acc: 0.3727 - val_loss: 1.5840 - val_acc: 0.4043

Epoch 00004: val_acc improved from 0.36927 to 0.40430, saving model to model_weights.h5
Epoch 5/50
225/225 [==============================] - 31s 139ms/step - loss: 1.5755 - acc: 0.3963 - val_loss: 1.4627 - val_acc: 0.4344

Epoch 00005: val_acc improved from 0.40430 to 0.43442, saving model to model_weights.h5
Epoch 6/50
225/225 [==============================] - 31s 137ms/step - loss: 1.5243 - acc: 0.4150 - val_loss: 1.5903 - val_acc: 0.4067

Epoch 00006: val_acc did not improve from 0.43442
Epoch 7/50
225/225 [==============================] - 31s 138ms/step - loss: 1.4730 - acc: 0.4319 - val_loss: 1.4850 - val_acc: 0.4435

Epoch 00007: val_acc improved from 0.43442 to 0.44350, saving model to model_weights.h5
Epoch 8/50
225/225 [==============================] - 31s 137ms/step - loss: 1.4258 - acc: 0.4529 - val_loss: 1.3847 - val_acc: 0.4706

Epoch 00008: val_acc improved from 0.44350 to 0.47060, saving model to model_weights.h5
Epoch 9/50
225/225 [==============================] - 30s 136ms/step - loss: 1.3888 - acc: 0.4659 - val_loss: 1.3654 - val_acc: 0.4836

Epoch 00009: val_acc improved from 0.47060 to 0.48357, saving model to model_weights.h5
Epoch 10/50
225/225 [==============================] - 31s 137ms/step - loss: 1.3576 - acc: 0.4785 - val_loss: 1.3149 - val_acc: 0.5027

Epoch 00010: val_acc improved from 0.48357 to 0.50274, saving model to model_weights.h5
Epoch 11/50
225/225 [==============================] - 31s 138ms/step - loss: 1.3179 - acc: 0.4940 - val_loss: 1.2621 - val_acc: 0.5144

Epoch 00011: val_acc improved from 0.50274 to 0.51441, saving model to model_weights.h5
Epoch 12/50
225/225 [==============================] - 30s 135ms/step - loss: 1.2952 - acc: 0.5055 - val_loss: 1.2465 - val_acc: 0.5255

Epoch 00012: val_acc improved from 0.51441 to 0.52551, saving model to model_weights.h5
Epoch 13/50
225/225 [==============================] - 30s 134ms/step - loss: 1.2654 - acc: 0.5178 - val_loss: 1.2080 - val_acc: 0.5344

Epoch 00013: val_acc improved from 0.52551 to 0.53445, saving model to model_weights.h5
Epoch 14/50
225/225 [==============================] - 30s 135ms/step - loss: 1.2445 - acc: 0.5247 - val_loss: 1.2140 - val_acc: 0.5301

Epoch 00014: val_acc did not improve from 0.53445
Epoch 15/50
225/225 [==============================] - 30s 133ms/step - loss: 1.2058 - acc: 0.5420 - val_loss: 1.1724 - val_acc: 0.5539

Epoch 00015: val_acc improved from 0.53445 to 0.55391, saving model to model_weights.h5
Epoch 16/50
225/225 [==============================] - 30s 135ms/step - loss: 1.1938 - acc: 0.5444 - val_loss: 1.1801 - val_acc: 0.5496

Epoch 00016: val_acc did not improve from 0.55391
Epoch 17/50
225/225 [==============================] - 30s 133ms/step - loss: 1.1699 - acc: 0.5546 - val_loss: 1.1988 - val_acc: 0.5425

Epoch 00017: val_acc did not improve from 0.55391
Epoch 18/50
225/225 [==============================] - 30s 133ms/step - loss: 1.1608 - acc: 0.5589 - val_loss: 1.1942 - val_acc: 0.5558

Epoch 00018: val_acc improved from 0.55391 to 0.55578, saving model to model_weights.h5
Epoch 19/50
225/225 [==============================] - 30s 133ms/step - loss: 1.1418 - acc: 0.5684 - val_loss: 1.1128 - val_acc: 0.5842

Epoch 00019: val_acc improved from 0.55578 to 0.58417, saving model to model_weights.h5
Epoch 20/50
225/225 [==============================] - 30s 135ms/step - loss: 1.1192 - acc: 0.5728 - val_loss: 1.0984 - val_acc: 0.5882

Epoch 00020: val_acc improved from 0.58417 to 0.58821, saving model to model_weights.h5
Epoch 21/50
225/225 [==============================] - 31s 138ms/step - loss: 1.0970 - acc: 0.5844 - val_loss: 1.1297 - val_acc: 0.5796

Epoch 00021: val_acc did not improve from 0.58821
Epoch 22/50
225/225 [==============================] - 30s 133ms/step - loss: 1.0842 - acc: 0.5886 - val_loss: 1.1092 - val_acc: 0.5824

Epoch 00022: val_acc did not improve from 0.58821
Epoch 23/50
225/225 [==============================] - 30s 134ms/step - loss: 1.0676 - acc: 0.5962 - val_loss: 1.0839 - val_acc: 0.5986

Epoch 00023: val_acc improved from 0.58821 to 0.59859, saving model to model_weights.h5
Epoch 24/50
225/225 [==============================] - 30s 135ms/step - loss: 1.0520 - acc: 0.6001 - val_loss: 1.1386 - val_acc: 0.5843

Epoch 00024: val_acc did not improve from 0.59859
Epoch 25/50
225/225 [==============================] - 30s 134ms/step - loss: 1.0287 - acc: 0.6091 - val_loss: 1.0872 - val_acc: 0.5927

Epoch 00025: val_acc did not improve from 0.59859
Epoch 26/50
225/225 [==============================] - 30s 133ms/step - loss: 1.0191 - acc: 0.6148 - val_loss: 1.0897 - val_acc: 0.6019

Epoch 00026: val_acc improved from 0.59859 to 0.60190, saving model to model_weights.h5
Epoch 27/50
225/225 [==============================] - 30s 135ms/step - loss: 0.9993 - acc: 0.6214 - val_loss: 1.1125 - val_acc: 0.5872

Epoch 00027: val_acc did not improve from 0.60190
Epoch 28/50
225/225 [==============================] - 30s 133ms/step - loss: 0.9827 - acc: 0.6258 - val_loss: 1.0542 - val_acc: 0.6081

Epoch 00028: val_acc improved from 0.60190 to 0.60810, saving model to model_weights.h5
Epoch 29/50
225/225 [==============================] - 30s 134ms/step - loss: 0.9668 - acc: 0.6356 - val_loss: 1.0570 - val_acc: 0.6035

Epoch 00029: val_acc did not improve from 0.60810
Epoch 30/50
225/225 [==============================] - 31s 137ms/step - loss: 0.9546 - acc: 0.6402 - val_loss: 1.0983 - val_acc: 0.5920

Epoch 00030: val_acc did not improve from 0.60810
Epoch 31/50
225/225 [==============================] - 30s 135ms/step - loss: 0.9343 - acc: 0.6511 - val_loss: 1.0527 - val_acc: 0.6162

Epoch 00031: val_acc improved from 0.60810 to 0.61617, saving model to model_weights.h5
Epoch 32/50
225/225 [==============================] - 30s 134ms/step - loss: 0.9192 - acc: 0.6531 - val_loss: 1.0602 - val_acc: 0.6127

Epoch 00032: val_acc did not improve from 0.61617
Epoch 33/50
225/225 [==============================] - 30s 133ms/step - loss: 0.9053 - acc: 0.6590 - val_loss: 1.0347 - val_acc: 0.6258

Epoch 00033: val_acc improved from 0.61617 to 0.62583, saving model to model_weights.h5
Epoch 34/50
225/225 [==============================] - 30s 133ms/step - loss: 0.8927 - acc: 0.6660 - val_loss: 1.0769 - val_acc: 0.6167

Epoch 00034: val_acc did not improve from 0.62583
Epoch 35/50
225/225 [==============================] - 30s 133ms/step - loss: 0.8725 - acc: 0.6726 - val_loss: 1.1031 - val_acc: 0.6064

Epoch 00035: val_acc did not improve from 0.62583
Epoch 36/50
225/225 [==============================] - 30s 133ms/step - loss: 0.8613 - acc: 0.6787 - val_loss: 1.0459 - val_acc: 0.6235

Epoch 00036: val_acc did not improve from 0.62583
Epoch 37/50
225/225 [==============================] - 30s 133ms/step - loss: 0.8457 - acc: 0.6806 - val_loss: 1.0133 - val_acc: 0.6303

Epoch 00037: val_acc improved from 0.62583 to 0.63030, saving model to model_weights.h5
Epoch 38/50
225/225 [==============================] - 30s 133ms/step - loss: 0.8316 - acc: 0.6862 - val_loss: 1.0442 - val_acc: 0.6258

Epoch 00038: val_acc did not improve from 0.63030
Epoch 39/50
225/225 [==============================] - 30s 134ms/step - loss: 0.8164 - acc: 0.6937 - val_loss: 1.0369 - val_acc: 0.6307

Epoch 00039: val_acc improved from 0.63030 to 0.63073, saving model to model_weights.h5
Epoch 40/50
225/225 [==============================] - 30s 135ms/step - loss: 0.8031 - acc: 0.7005 - val_loss: 1.0461 - val_acc: 0.6225

Epoch 00040: val_acc did not improve from 0.63073
Epoch 41/50
225/225 [==============================] - 30s 134ms/step - loss: 0.7930 - acc: 0.7040 - val_loss: 1.0584 - val_acc: 0.6284

Epoch 00041: val_acc did not improve from 0.63073
Epoch 42/50
225/225 [==============================] - 30s 132ms/step - loss: 0.7756 - acc: 0.7081 - val_loss: 1.0227 - val_acc: 0.6466

Epoch 00042: val_acc improved from 0.63073 to 0.64658, saving model to model_weights.h5
Epoch 43/50
225/225 [==============================] - 30s 134ms/step - loss: 0.7557 - acc: 0.7165 - val_loss: 1.0745 - val_acc: 0.6178

Epoch 00043: val_acc did not improve from 0.64658
Epoch 44/50
225/225 [==============================] - 30s 133ms/step - loss: 0.7465 - acc: 0.7208 - val_loss: 1.0477 - val_acc: 0.6400

Epoch 00044: val_acc did not improve from 0.64658
Epoch 45/50
225/225 [==============================] - 31s 137ms/step - loss: 0.7272 - acc: 0.7297 - val_loss: 1.0677 - val_acc: 0.6343

Epoch 00045: val_acc did not improve from 0.64658
Epoch 46/50
225/225 [==============================] - 30s 133ms/step - loss: 0.7147 - acc: 0.7338 - val_loss: 1.0639 - val_acc: 0.6473

Epoch 00046: val_acc improved from 0.64658 to 0.64730, saving model to model_weights.h5
Epoch 47/50
225/225 [==============================] - 30s 133ms/step - loss: 0.6962 - acc: 0.7423 - val_loss: 1.0351 - val_acc: 0.6443

Epoch 00047: val_acc did not improve from 0.64730
Epoch 48/50
225/225 [==============================] - 30s 133ms/step - loss: 0.6868 - acc: 0.7454 - val_loss: 1.0418 - val_acc: 0.6436

Epoch 00048: val_acc did not improve from 0.64730
Epoch 49/50
225/225 [==============================] - 30s 133ms/step - loss: 0.6783 - acc: 0.7461 - val_loss: 1.0289 - val_acc: 0.6502

Epoch 00049: val_acc improved from 0.64730 to 0.65019, saving model to model_weights.h5
Epoch 50/50
225/225 [==============================] - 30s 135ms/step - loss: 0.6632 - acc: 0.7545 - val_loss: 1.0649 - val_acc: 0.6430

Epoch 00050: val_acc did not improve from 0.65019
CPU times: user 24min 25s, sys: 6min 33s, total: 30min 59s
Wall time: 26min 8s
Our best model managed to obtain a validation accuracy of approximately 65%, which is quite good given the fact that our target class has 7 possible values!

At each epoch, Keras checks if our model performed better than during the previous epochs. If it is the case, the new best model weights are saved into a file. This will allow us to load directly the weights of our model without having to re-train it if we want to use it.

We also have to save the structure of our CNN (layers etc.) into a file:

# serialize model structure to JSON
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
Analyze the results
We got outputs at each step of the training phase. All those outputs were saved into the 'history' variable. We can use it to plot the evolution of the loss and accuracy on both the train and validation datasets:

# plot the evolution of Loss and Acuracy on the train and validation sets

import matplotlib.pyplot as plt

plt.figure(figsize=(20,10))
plt.subplot(1, 2, 1)
plt.suptitle('Optimizer : Adam', fontsize=10)
plt.ylabel('Loss', fontsize=16)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend(loc='upper right')

plt.subplot(1, 2, 2)
plt.ylabel('Accuracy', fontsize=16)
plt.plot(history.history['acc'], label='Training Accuracy')
plt.plot(history.history['val_acc'], label='Validation Accuracy')
plt.legend(loc='lower right')
plt.show()

The validation accuracy starts to stabilize at the end of the 50 epochs between 60% and 65% accuracy.

The training loss is slightly higher than the validation loss for the first epochs which can be surprising. Indeed we are more used to see higher validation losses than training losses in machine learning. Here this is simply due to the presence of dropout, which is only applied during the training phase and not during the validation phase.

We can see that the training loss is becoming much smaller than the validation loss after the 20th epochs. This means that our model starts to overfit our training dataset after too much iterations. That is why the validation loss does not decrease a lot after. One solution consists in early-stopping the training of the model.

We could also use some different dropout values and performing data augmentation. Those methods were tested on this dataset, but they did not significantly increase the validation accuracy although they reduced the overfitting effect. Using them slightly increased the training duration of the model.

Finally we can plot the confusion matrix in order to see how our model classified the images:

# show the confusion matrix of our predictions

# compute predictions
predictions = model.predict_generator(generator=validation_generator)
y_pred = [np.argmax(probas) for probas in predictions]
y_test = validation_generator.classes
class_names = validation_generator.class_indices.keys()

from sklearn.metrics import confusion_matrix
import itertools

def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    plt.figure(figsize=(10,10))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    
# compute confusion matrix
cnf_matrix = confusion_matrix(y_test, y_pred)
np.set_printoptions(precision=2)

# plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')
plt.show()
<Figure size 432x288 with 0 Axes>

Our model is very good for predicting happy and surprised faces. However it predicts quite poorly feared faces because it confuses them with sad faces.

With more research and more resources this model could certainly be improved, but the goal of this study was primarily to focus on obtaining a fairly good model compared to what has been done in this field.

Now it's time to try our model in a real situation! We will use flask to serve our model in order to perform real-time predictions with a webcam input.
